{
	"class": "vulnerability",
	"id": "oval:com.ubuntu.oracular:def:2024531690000000",
	"version": "1",
	"metadata": {
		"title": "CVE-2024-53169 on Ubuntu 24.10 (oracular) - medium",
		"description": "In the Linux kernel, the following vulnerability has been resolved:nvme-fabrics: fix kernel crash while shutting down controllerThe nvme keep-alive operation, which executes at a periodic interval,could potentially sneak in while shutting down a fabric controller.This may lead to a race between the fabric controller admin queuedestroy code path (invoked while shutting down controller) and hw/hctxqueue dispatcher called from the nvme keep-alive async request queuingoperation. This race could lead to the kernel crash shown below:Call Trace:    autoremove_wake_function+0x0/0xbc (unreliable)    __blk_mq_sched_dispatch_requests+0x114/0x24c    blk_mq_sched_dispatch_requests+0x44/0x84    blk_mq_run_hw_queue+0x140/0x220    nvme_keep_alive_work+0xc8/0x19c [nvme_core]    process_one_work+0x200/0x4e0    worker_thread+0x340/0x504    kthread+0x138/0x140    start_kernel_thread+0x14/0x18While shutting down fabric controller, if nvme keep-alive request sneaksin then it would be flushed off. The nvme_keep_alive_end_io function isthen invoked to handle the end of the keep-alive operation whichdecrements the admin->q_usage_counter and assuming this is the last/onlyrequest in the admin queue then the admin->q_usage_counter becomes zero.If that happens then blk-mq destroy queue operation (blk_mq_destroy_queue()) which could be potentially running simultaneously on anothercpu (as this is the controller shutdown code path) would forwardprogress and deletes the admin queue. So, now from this point onwardwe are not supposed to access the admin queue resources. However theissue here's that the nvme keep-alive thread running hw/hctx queuedispatch operation hasn't yet finished its work and so it could stillpotentially access the admin queue resource while the admin queue hadbeen already deleted and that causes the above crash.The above kernel crash is regression caused due to changes implementedin commit a54a93d0e359 (\"nvme: move stopping keep-alive intonvme_uninit_ctrl()\"). Ideally we should stop keep-alive before destroying the admin queue and freeing the admin tagset so that it wouldn't sneakin during the shutdown operation. However we removed the keep alive stopoperation from the beginning of the controller shutdown code path in commita54a93d0e359 (\"nvme: move stopping keep-alive into nvme_uninit_ctrl()\")and added it under nvme_uninit_ctrl() which executes very late in theshutdown code path after the admin queue is destroyed and its tagset isremoved. So this change created the possibility of keep-alive sneaking inand interfering with the shutdown operation and causing observed kernelcrash.To fix the observed crash, we decided to move nvme_stop_keep_alive() fromnvme_uninit_ctrl() to nvme_remove_admin_tag_set(). This change would ensurethat we don't forward progress and delete the admin queue until the keep-alive operation is finished (if it's in-flight) or cancelled and that wouldhelp contain the race condition explained above and hence avoid the crash.Moving nvme_stop_keep_alive() to nvme_remove_admin_tag_set() instead ofadding nvme_stop_keep_alive() to the beginning of the controller shutdowncode path in nvme_stop_ctrl(), as was the case earlier before commita54a93d0e359 (\"nvme: move stopping keep-alive into nvme_uninit_ctrl()\"),would help save one callsite of nvme_stop_keep_alive().",
		"affected": {
			"family": "unix",
			"platform": "Ubuntu 24.10"
		},
		"reference": {
			"source": "CVE",
			"ref_id": "CVE-2024-53169",
			"ref_url": "https://www.cve.org/CVERecord?id=CVE-2024-53169"
		},
		"advisory": {
			"severity": "Medium",
			"rights": "Copyright (C) 2024 Canonical Ltd.",
			"public_date": "2024-12-27 14:15:00 UTC",
			"cve": {
				"text": "CVE-2024-53169",
				"href": "https://ubuntu.com/security/CVE-2024-53169",
				"public": "20241227"
			}
		}
	},
	"notes": {},
	"criteria": {
		"criterias": [
			{
				"operator": "OR",
				"criterions": [
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000000",
						"comment": "linux package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-17.17')."
					},
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000010",
						"comment": "linux-aws package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-1009.10')."
					},
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000020",
						"comment": "linux-azure package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-1009.9')."
					},
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000030",
						"comment": "linux-gcp package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-1009.9')."
					},
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000060",
						"comment": "linux-lowlatency package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-1009.10')."
					},
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000040",
						"comment": "linux-oracle package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-1011.12')."
					},
					{
						"test_ref": "oval:com.ubuntu.oracular:tst:201245420000050",
						"comment": "linux-raspi package in oracular is affected. An update containing the fix has been completed and is pending publication (note: '6.11.0-1008.8')."
					}
				]
			}
		]
	}
}
